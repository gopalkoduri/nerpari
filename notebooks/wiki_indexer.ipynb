{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Run this file from the place where wiki dump is extracted\n",
      "\n",
      "data_dir = \"/homedtic/gkoduri/data/wiki/extracted\"\n",
      "code_dir = \"/homedtic/gkoduri/workspace/relation-extraction\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from glob import glob\n",
      "import sys\n",
      "import codecs\n",
      "import pickle\n",
      "from uuid import uuid5, NAMESPACE_URL\n",
      "from os.path import basename\n",
      "from BeautifulSoup import BeautifulSoup\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_page_index(arg):\n",
      "    index = {}\n",
      "    \n",
      "    files = glob(data_dir + \"/\" + arg + \"/wiki*\")\n",
      "    identifier = str(uuid5(NAMESPACE_URL, arg))\n",
      "    \n",
      "    log_file = code_dir + \"/data/\" + identifier + \"_log.txt\"\n",
      "    log = codecs.open(log_file, \"w\")\n",
      "    \n",
      "    for f in files:\n",
      "        print f\n",
      "        data = codecs.open(f, 'r', 'utf-8').readlines()\n",
      "        size = len(data)\n",
      "        step = 100\n",
      "        for ind in xrange(0, size, step):\n",
      "            try:\n",
      "                soup = BeautifulSoup(\"\".join(data[ind:ind+step]))\n",
      "            except UnicodeEncodeError, UnicodeDecodeError:\n",
      "                log.write(f + \"\\t\" + str(ind) + \"\\n\")\n",
      "            pages = soup.findAll('doc')\n",
      "            for page in pages:\n",
      "                page_title = page.attrs[2][1]\n",
      "                index[page_title.lower()] = arg + \"/\" + basename(f)\n",
      "    \n",
      "    log.close()\n",
      "    \n",
      "    index_file = code_dir + \"/data/\" + identifier + \".pickle\"\n",
      "    pickle.dump(index, file(index_file, \"w\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_link_index(wiki_folder_arg, music_style):\n",
      "    link_index = {}\n",
      "    \n",
      "    files = glob(data_dir + \"/\" + wiki_folder_arg + \"/wiki*\")\n",
      "    identifier = str(uuid5(NAMESPACE_URL, wiki_folder_arg))\n",
      "    \n",
      "    log_file = code_dir + \"/data/wiki_link_index_logs/\" + identifier + \"_log.txt\"\n",
      "    log = codecs.open(log_file, \"w\")\n",
      "    \n",
      "    for f in files:\n",
      "        print f\n",
      "        data = codecs.open(f, 'r', 'utf-8').read()\n",
      "        try:\n",
      "            soup = BeautifulSoup(data.lower())\n",
      "        except UnicodeEncodeError, UnicodeDecodeError:\n",
      "            log.write(f + \"\\n\")\n",
      "            continue\n",
      "        pages = soup.findAll('doc')\n",
      "        for page in pages:\n",
      "            if music_style not in page.text:\n",
      "                continue\n",
      "            page_title = page.attrs[2][1]\n",
      "            links = page.findAll(\"a\")\n",
      "            link_terms = [link.text.lower() for link in links]\n",
      "            link_index[page_title.lower()] = np.unique(link_terms).tolist()\n",
      "    \n",
      "    log.close()\n",
      "    \n",
      "    link_index_file = code_dir + \"/data/wiki_link_index/\" + identifier + \".pickle\"\n",
      "    pickle.dump(link_index, file(link_index_file, \"w+\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def merge_indexes(files):\n",
      "    whole_index = {}\n",
      "    for f in files:\n",
      "        print f\n",
      "        data = pickle.load(file(f))\n",
      "        whole_index.update(data)\n",
      "        \n",
      "    return whole_index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if __name__ == \"__main__\":\n",
      "    #build_page_index(sys.argv[1])\n",
      "    \n",
      "    #files = glob(code_dir + \"/data/wiki_index/*.pickle\")\n",
      "    #whole_index = merge_indexes(files)\n",
      "    #pickle.dump(whole_index, file(code_dir + \"/data/wiki_index.pickle\", \"w\"))\n",
      "    \n",
      "    import sys\n",
      "    for folder in sys.argv[1:]:\n",
      "        build_link_index(folder, \"jazz\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}