{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Notebook for weighted graph analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For all the pythonic graph modifications, we use networkx as it is easy! On the other hand, graph-tool is completely written in C++ and is orders of magnitude faster than networkx. So we use it for computations on graphs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "\n",
      "import graph_tool.all as gt\n",
      "import networkx as nx\n",
      "import community as c\n",
      "import pickle\n",
      "import numpy as np\n",
      "\n",
      "#matplotlib tk\n",
      "#rcParams['figure.figsize'] = 16, 12"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fname = '/homedtic/gkoduri/workspace/relation-extraction/data/carnatic_cocitation.graphml'\n",
      "nx_g = nx.read_graphml(fname, node_type=unicode)\n",
      "print nx_g.number_of_nodes(), nx_g.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "884 25688\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gt_g = gt.load_graph(file_name=fname, fmt='xml')\n",
      "print gt_g.num_vertices(), gt_g.num_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Eigenvector centrality filter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eig_data = gt.eigenvector(gt_g, weight=gt_g.edge_properties[\"weight\"])\n",
      "#eig_data[0] is the largest eigenvalue of weighted adjacency matrix\n",
      "#eig_data[1] has the vertex property map with eigenvector centralities for each vertex\n",
      "print eig_data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#covert the eig values to be used easily with networkx\n",
      "gt_labelmap = gt_g.vertex_properties['_graphml_vertex_id']\n",
      "eig_centralities = {}\n",
      "for v in gt_g.vertices():\n",
      "    eig_centralities[gt_labelmap[v].decode('utf-8')] = eig_data[1][v]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Just have a look at some values!**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nodes = [\"m. s. subbulakshmi\", \"t. m. krishna\", \"carnatic wars\", \"abhogi\", \"telugu literature\", \n",
      "         \"s. p. balasubrahmanyam\", \"kingdom of mysore\", \"philip v. francis\"]\n",
      "\n",
      "print [(n, eig_centralities[n]) for n in nodes if n in eig_centralities.keys()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Plot how the number of relations per node varies as we keep increasing the eig_centrality threshold to remove spurious nodes.** "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tempg = nx_g.copy()\n",
      "eig_limits = np.arange(0.00001, 0.1, 0.0003)\n",
      "edges = []\n",
      "nodes = []\n",
      "y = []\n",
      "for eig_limit in eig_limits:\n",
      "    nbunch = [n for n,v in eig_centralities.items() if v < eig_limit]\n",
      "    tempg.remove_nodes_from(nbunch)\n",
      "    \n",
      "    edges.append(tempg.number_of_edges())\n",
      "    nodes.append(tempg.number_of_nodes())\n",
      "    if nodes[-1] == 0: break\n",
      "    y.append(1.0*edges[-1]/nodes[-1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(eig_limits[:len(y)], y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* **Get the eig_value corresponding to the peak in that plot.**\n",
      "* **Get the nodes setting that eig_value as threshold. These are the nodes which we are most confident are relevant!**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "thresh = 0.0009\n",
      "nodes = [i for i in nx_g.nodes_iter() if eig_centralities[i] > thresh]\n",
      "central_nodes = nodes\n",
      "print len(central_nodes)\n",
      "central_nodes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Evalution of eigenvector centrality's performance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotations = pickle.load(file('/homedtic/gkoduri/workspace/relation-extraction/data/annotations.pickle'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = []\n",
      "precisions = []\n",
      "recalls = []\n",
      "f_measures = []\n",
      "eig_limits = np.arange(0.00001, 0.5, 0.0003)\n",
      "\n",
      "for eig_limit in eig_limits:\n",
      "    tp = 0.0\n",
      "    tn = 0.0\n",
      "    fp = 0.0\n",
      "    fn = 0.0\n",
      "\n",
      "    nodes = [i for i in nx_g.nodes_iter() if eig_centralities[i] > eig_limit]\n",
      "    \n",
      "    if len(nodes) <= 50:\n",
      "        break\n",
      "        \n",
      "    n.append(len(nodes))\n",
      "    for page, category in annotations.items():\n",
      "        if page in nodes:\n",
      "            if category == \"carnatic\":\n",
      "                tp += 1\n",
      "            else:\n",
      "                fp += 1\n",
      "        else:\n",
      "            if category == \"carnatic\":\n",
      "                fn += 1\n",
      "            else:\n",
      "                tn += 1\n",
      "    precisions.append(tp/(tp+fp))\n",
      "    recalls.append(tp/(tp+fn))\n",
      "    f_measures.append(2*tp/(2*tp+fp+fn))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = array(n)/nx_g.number_of_nodes()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = eig_limits[:len(n)]\n",
      "\n",
      "plot(x, n, label=\"Number of nodes\")\n",
      "plot(x, f_measures, label=\"F-measure\")\n",
      "plot(x, precisions, label=\"Precision\")\n",
      "plot(x, recalls, label=\"Recalls\")\n",
      "\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Different properties of graph to know its structure\n",
      "\n",
      "**What are the properties of the graph with only these central nodes, and how do they compare to the original graph?**\n",
      "\n",
      "* Diameter (nx.diameter)\n",
      "* Size (number of edges)\n",
      "* Clustering coefficient (nx.average_clustering)\n",
      "* Maximal independent set (nx.maximal_independent_set) --> *needs to be run multiple times and an average taken*\n",
      "* Clique number (nx.clique.graph_clique_number)\n",
      "* Algebraic connectivity (Second smallest eigen value of laplacian matrix of g)\n",
      "* Cheeger constant (TODO)\n",
      "* Minimum node/edge cut (nx.minimum_edge_cut or nx.minimum_node_cut)\n",
      "* Matchings (nx.matching.maximal_matching)\n",
      "* Estrada index (nx.estrada_index)\n",
      "* Vertex cover (networkx_approximate.py)\n",
      "\n",
      "\n",
      "**Other measures that can be considered when adding less good nodes to central nodes:**\n",
      "\n",
      "* Closeness vitality (related to weiner index, nx.closeness_vitality) --> *Is too slow!*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import cStringIO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def propmap_len(vertices, propmap):\n",
      "    count = 0\n",
      "    for v in vertices:\n",
      "        if propmap[v]:\n",
      "            count += 1\n",
      "    return count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Convert graphs from networkx to graph-tool format**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convert_graph(src_g, to=\"graphtool\"):\n",
      "    buf = cStringIO.StringIO()\n",
      "    if to == \"graphtool\":\n",
      "        nx.write_graphml(src_g, buf, encoding='utf-8')\n",
      "        buf.flush()\n",
      "        buf.reset()\n",
      "        dest_g = gt.Graph()\n",
      "        dest_g.load(buf, fmt=\"xml\")\n",
      "    else:\n",
      "        src_g.save(buf, fmt=\"xml\")\n",
      "        buf.flush()\n",
      "        buf.reset()\n",
      "        dest_g = nx.read_graphml(buf, node_type=unicode)\n",
      "    return dest_g"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cg = nx_g.subgraph(nodes)\n",
      "cg = convert_graph(cg)\n",
      "\n",
      "central_g = nx_g.subgraph(central_nodes)\n",
      "central_g = convert_graph(central_g)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Diameter**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res_central = gt.pseudo_diameter(central_g)\n",
      "res_ext = gt.pseudo_diameter(cg)\n",
      "res_all = gt.pseudo_diameter(gt_g)\n",
      "print res_central[0], res_ext[0], res_all[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Maximal independent vertex set**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = gt.max_independent_vertex_set(central_g)\n",
      "print propmap_len(central_g.vertices(), res)\n",
      "\n",
      "res = gt.max_independent_vertex_set(cg)\n",
      "print propmap_len(cg.vertices(), res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Maximal matching**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = gt.max_cardinality_matching(central_g)\n",
      "#print propmap_len(central_g.edges(), res)\n",
      "print sum(res[0].a)\n",
      "\n",
      "res = gt.max_cardinality_matching(cg)\n",
      "print sum(res[0].a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Algebraic connectivity**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lap = gt.laplacian(central_g)\n",
      "eig_vals = np.linalg.eigvals(lap.todense())\n",
      "eig_vals = sort(eig_vals)\n",
      "print eig_vals[1],\n",
      "\n",
      "lap = gt.laplacian(cg)\n",
      "eig_vals = np.linalg.eigvals(lap.todense())\n",
      "eig_vals = sort(eig_vals)\n",
      "print eig_vals[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Min cut**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print gt.min_cut(central_g, central_g.edge_properties[\"weight\"])[0] #, len(nx.minimum_edge_cut(cg))\n",
      "print gt.min_cut(cg, cg.edge_properties[\"weight\"])[0] #, len(nx.minimum_node_cut(cg))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Plot how all these measures vary as the number of nodes are increased**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eig_limits = np.arange(0.025, 0.0001, -0.0003)\n",
      "edges = []\n",
      "nodes = []\n",
      "diameters = []\n",
      "maxind_vertsets = []\n",
      "max_matches = []\n",
      "alg_conn = []\n",
      "min_cut = []\n",
      "\n",
      "for eig_limit in eig_limits:\n",
      "    print eig_limit\n",
      "    \n",
      "    nbunch = [n for n,v in eig_centralities.items() if v < eig_limit]\n",
      "    cg = nx_g.subgraph(nbunch)\n",
      "    cg = convert_graph(cg, to=\"graphtool\")\n",
      "    \n",
      "    edges.append(cg.num_edges())\n",
      "    nodes.append(cg.num_vertices())\n",
      "    if nodes[-1] == 0: \n",
      "        break\n",
      "    \n",
      "    diameters.append(gt.pseudo_diameter(cg)[0])\n",
      "    \n",
      "    res = gt.max_independent_vertex_set(cg)\n",
      "    maxind_vertsets.append(propmap_len(cg.vertices(), res))\n",
      "    \n",
      "    res = gt.max_cardinality_matching(cg)\n",
      "    max_matches.append(sum(res[0].a))\n",
      "    \n",
      "    lap = gt.laplacian(cg)\n",
      "    eig_vals = np.linalg.eigvals(lap.todense())\n",
      "    eig_vals = sort(eig_vals)\n",
      "    alg_conn.append(eig_vals[1])\n",
      "    \n",
      "    min_cut.append(gt.min_cut(cg, cg.edge_properties[\"weight\"])[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "min_cut = []\n",
      "clust_coeff = []\n",
      "clust_coeff_std = []\n",
      "for eig_limit in eig_limits:\n",
      "    print eig_limit,\n",
      "    nbunch = [n for n,v in eig_centralities.items() if v < eig_limit]\n",
      "    cg = nx_g.subgraph(nbunch)\n",
      "    for u,v,d in cg.edges(data=True):\n",
      "        d[\"weight\"] = 1\n",
      "    cg = convert_graph(cg, to=\"graphtool\")\n",
      "    \n",
      "    clust_coeff.append(gt.global_clustering(cg)[0])\n",
      "    clust_coeff_std.append(gt.global_clustering(cg)[1])\n",
      "    \n",
      "    #res = gt.min_cut(cg, cg.edge_properties[\"weight\"])[0]\n",
      "    #min_cut.append(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nbunch = [n for n,v in eig_centralities.items() if v < 0.001]\n",
      "cg = nx_g.subgraph(nbunch)\n",
      "for u,v,d in cg.edges(data=True):\n",
      "    d[\"weight\"] = 1\n",
      "print cg.number_of_nodes(), cg.number_of_edges()\n",
      "\n",
      "cg = convert_graph(cg)\n",
      "print cg.num_vertices(), cg.num_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for e in cg.edges():\n",
      "    print e, cg.edge_properties[\"weight\"][e]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "res = gt.min_cut(cg, cg.edge_properties[\"weight\"])\n",
      "for v in cg.vertices():\n",
      "    if res[1][v]:\n",
      "        print v"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y = min_cut\n",
      "\n",
      "#res = zip(eig_limits, y)\n",
      "#res = array(sorted(res, key=lambda x:x[0]))\n",
      "#plot(res[:, 0], res[:, 1])\n",
      "\n",
      "plot(eig_limits, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_graphml(cg, \"/homedtic/gkoduri/workspace/relation-extraction/data/carnatic_link_graph_0.1_cg.graphml\", \n",
      "                 encoding=\"utf-8\")\n",
      "nx.write_graphml(central_g, \"/homedtic/gkoduri/workspace/relation-extraction/data/carnatic_link_graph_0.1_central_g.graphml\", \n",
      "                 encoding=\"utf-8\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Filtering by communities\n",
      "\n",
      "* Get the communities from the graph, setting a constraint of minimum number of nodes per community."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "curg = nx_g.copy()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Remove too weak edges "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for u,v,d in curg.edges(data=True):\n",
      "    if d[\"weight\"] < 0.1:\n",
      "        curg.remove_edge(u,v)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print curg.number_of_nodes(), curg.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dendo = c.generate_dendogram(curg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(len(dendo)):\n",
      "    partition = c.partition_at_level(dendo, i)\n",
      "    modularity = c.modularity(partition, curg)\n",
      "    num_parts = len(np.unique(partition.values()))\n",
      "    print i, modularity, num_parts\n",
      "    \n",
      "    categories = {}\n",
      "    min_com_size = 10\n",
      "    \n",
      "    for com in set(partition.values()) :\n",
      "        list_nodes = [nodes for nodes in partition.keys()\n",
      "                                    if partition[nodes] == com]\n",
      "        \n",
      "        if len(list_nodes) > min_com_size:\n",
      "            categories[com] = list_nodes\n",
      "            print com, len(list_nodes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 0.591416226406 87\n",
        "0 93\n",
        "1 156\n",
        "2 61\n",
        "4 33\n",
        "6 21\n",
        "7 110\n",
        "8 11\n",
        "9 25\n",
        "11 29\n",
        "13 53\n",
        "16 12\n",
        "20 12\n",
        "35 12\n",
        "37 13\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.61131293072 40\n",
        "0 105\n",
        "1 222\n",
        "2 103\n",
        "3 17\n",
        "4 100\n",
        "5 125\n",
        "6 30\n",
        "7 48\n",
        "8 15\n",
        "10 14\n",
        "11 27\n",
        "17 14\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.611624700246 38\n",
        "0 105\n",
        "1 222\n",
        "2 103\n",
        "3 65\n",
        "4 100\n",
        "5 125\n",
        "6 30\n",
        "7 29\n",
        "9 14\n",
        "10 27\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = {}\n",
      "min_com_size = 10\n",
      "\n",
      "for com in set(partition.values()) :\n",
      "    list_nodes = [nodes for nodes in partition.keys()\n",
      "                                if partition[nodes] == com]\n",
      "    print com, len(list_nodes)\n",
      "    print\n",
      "    if len(list_nodes) > min_com_size:\n",
      "         categories[com] = list_nodes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for com, nbunch in categories.items():\n",
      "    print com, len(nbunch), sort(nbunch)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2.1 Cleaning with the help of a graph metric"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#eigenvector centrality\n",
      "for com, nbunch in categories.items():\n",
      "    print com, len(nbunch), sum([ eig_centralities[n] for n in nbunch if n in eig_centralities.keys() ])/len(nbunch)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#maximal independent set\n",
      "for com, nbunch in categories.items():\n",
      "    cg = curg.subgraph(nbunch)\n",
      "    cg = convert_graph(cg)\n",
      "    res = gt.max_independent_vertex_set(cg)\n",
      "    print com, len(nbunch), propmap_len(cg.vertices(), res), propmap_len(cg.vertices(), res)/len(nbunch)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for com, nbunch in categories.items():\n",
      "    cg = curg.subgraph(nbunch)\n",
      "    cg = convert_graph(cg)\n",
      "    res = gt.max_cardinality_matching(cg)\n",
      "    print com, len(nbunch), sum(res[0].a), sum(res[0].a)/len(nbunch)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#algebraic connectivity\n",
      "for com, nbunch in categories.items():\n",
      "    cg = curg.subgraph(nbunch)\n",
      "\n",
      "    #lap = gt.laplacian(cg)\n",
      "    #eig_vals = np.linalg.eigvals(lap.todense())\n",
      "    lap = nx.linalg.laplacian_matrix(cg)\n",
      "    eig_vals = np.linalg.eigvals(lap)\n",
      "    eig_vals = sort(eig_vals)\n",
      "    print com, len(nbunch), eig_vals[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for com, nbunch in categories.items():\n",
      "    cg = curg.subgraph(nbunch)\n",
      "    cg = convert_graph(cg)\n",
      "    print com, len(nbunch), round(gt.pseudo_diameter(cg, weights=cg.edge_properties[\"weight\"])[0], 2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Using disparity filter"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_graph_edges(G, DISPARITY_FILTER_SIGNIF_LEVEL, verbose=False, print_prefix=''):\n",
      "\n",
      "    '''\n",
      "    A large number of complex systems find a natural abstraction in the form of weighted networks whose nodes represent\n",
      "    the elements of the system and the weighted edges identify the presence of an interaction and its relative strength.\n",
      "    In recent years, the study of an increasing number of large-scale networks has highlighted the statistical\n",
      "    heterogeneity of their interaction pattern, with degree and weight distributions that vary over many orders of\n",
      "    magnitude. These features, along with the large number of elements and links, make the extraction of the truly\n",
      "    relevant connections forming the network's backbone a very challenging problem. More specifically, coarse-graining\n",
      "    approaches and filtering techniques come into conflict with the multiscale nature of large-scale systems. Here, we\n",
      "    define a filtering method that offers a practical procedure to extract the relevant connection backbone in complex\n",
      "    multiscale networks, preserving the edges that represent statistically significant deviations with respect to a\n",
      "    null model for the local assignment of weights to edges. An important aspect of the method is that it does not\n",
      "    belittle small-scale interactions and operates at all scales defined by the weight distribution. We apply our\n",
      "    method to real-world network instances and compare the obtained results with alternative backbone\n",
      "    extraction techniques. (http://www.pnas.org/content/106/16/6483.abstract)\n",
      "    '''\n",
      "\n",
      "    if verbose:\n",
      "        print '%sFiltering with ' % print_prefix + str(100*(1-DISPARITY_FILTER_SIGNIF_LEVEL))+'% confidence ...',\n",
      "\n",
      "    # FOR DIRECTED\n",
      "    #indegree = G.in_degree(weight=None)\n",
      "    #outdegree = G.out_degree(weight=None)\n",
      "    #instrength = G.in_degree(weight='weight')\n",
      "    #outstrength = G.out_degree(weight='weight')\n",
      "\n",
      "    # FOR UNDIRECTED\n",
      "    degree = G.degree(weight=None)\n",
      "    strength = G.degree(weight='weight')\n",
      "\n",
      "    edges = G.edges()\n",
      "    for i, j in edges:\n",
      "            # FOR DIRECTED\n",
      "            #pij = float(G[i][j]['weight'])/float(outstrength[i])\n",
      "            #pji = float(G[i][j]['weight'])/float(instrength[j])\n",
      "            #aij = (1-pij)**(outdegree[i]-1)\n",
      "            #aji = (1-pji)**(indegree[j]-1)\n",
      "            #if aij < DISPARITY_FILTER_SIGNIF_LEVEL or aji < DISPARITY_FILTER_SIGNIF_LEVEL:\n",
      "            #    continue\n",
      "\n",
      "            # FOR UNDIRECTED\n",
      "            pij = float(G[i][j]['weight'])/float(strength[i])\n",
      "            aij = (1-pij)**(degree[i]-1)\n",
      "            if aij < DISPARITY_FILTER_SIGNIF_LEVEL:\n",
      "                continue\n",
      "\n",
      "            G.remove_edge(i, j)\n",
      "    nodes = G.nodes()\n",
      "    for n in nodes:\n",
      "        if G.degree(n) < 1:\n",
      "            #print n\n",
      "            G.remove_node(n)\n",
      "    if verbose:\n",
      "        print G.number_of_nodes(), 'nodes,', G.number_of_edges(), 'edges'\n",
      "\n",
      "    return G\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_nodes = []\n",
      "num_edges = []\n",
      "precisions = []\n",
      "recalls = []\n",
      "f_measures = []\n",
      "\n",
      "disp_sig_levels = np.arange(0.01, 0.9, 0.01)\n",
      "\n",
      "for disp_sig_level in disp_sig_levels:\n",
      "    tp = 0.0\n",
      "    tn = 0.0\n",
      "    fp = 0.0\n",
      "    fn = 0.0\n",
      "\n",
      "    tempg = curg.copy()\n",
      "    filt_g = filter_graph_edges(tempg, disp_sig_level)\n",
      "        \n",
      "    num_nodes.append(filt_g.number_of_nodes())\n",
      "    num_edges.append(filt_g.number_of_edges())\n",
      "    \n",
      "    nodes = filt_g.nodes()\n",
      "    for page, category in annotations.items():\n",
      "        if page in nodes:\n",
      "            if category == \"carnatic\":\n",
      "                tp += 1\n",
      "            else:\n",
      "                fp += 1\n",
      "        else:\n",
      "            if category == \"carnatic\":\n",
      "                fn += 1\n",
      "            else:\n",
      "                tn += 1\n",
      "    if tp > 0:\n",
      "        precisions.append(tp/(tp+fp))\n",
      "        recalls.append(tp/(tp+fn))\n",
      "        f_measures.append(2*tp/(2*tp+fp+fn))\n",
      "    else:\n",
      "        precisions.append(0)\n",
      "        recalls.append(0)\n",
      "        f_measures.append(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = array(num_nodes)/curg.number_of_nodes()\n",
      "e = array(num_edges)/curg.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(disp_sig_levels, n, label=\"Number of nodes\")\n",
      "plot(disp_sig_levels, e, label=\"Number of edges\")\n",
      "plot(disp_sig_levels, f_measures, label=\"F-measure\")\n",
      "plot(disp_sig_levels, precisions, label=\"Precision\")\n",
      "plot(disp_sig_levels, recalls, label=\"Recalls\")\n",
      "\n",
      "legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Draw the partitions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_communities(g, partition):\n",
      "    colors = [\"#00308F\", \"#AF002A\", \"#FF7E00\", \"#665D1E\", \"#841B2D\", \"#3D2B1F\", \"#79443B\", \"#CD7F32\", \"#7FFF00\", \n",
      "              \"#893F45\", \"#9400D3\", \"#9B7653\", \"#555D50\", \"#CD5C5C\", \"#EE82EE\"]\n",
      "    \n",
      "    size = float(len(set(partition.values())))\n",
      "    pos = nx.spring_layout(g)\n",
      "    count = 0\n",
      "    for com in set(partition.values()) :\n",
      "        count = count + 1\n",
      "        list_nodes = [nodes for nodes in partition.keys()\n",
      "                                    if partition[nodes] == com]\n",
      "        nx.draw_networkx_nodes(g, pos, list_nodes, node_size = 20,\n",
      "                                    node_color = colors[count-1])\n",
      "    \n",
      "    \n",
      "    nx.draw_networkx_edges(g, pos, alpha=0.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_nodes(g, min_com_size=25):\n",
      "    partition = c.best_partition(g)\n",
      "    communities = {}\n",
      "    \n",
      "    for com in set(partition.values()) :\n",
      "        list_nodes = [nodes for nodes in partition.keys()\n",
      "                                    if partition[nodes] == com]\n",
      "        if len(list_nodes) > min_com_size:\n",
      "             communities[com] = list_nodes\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Shortest path algorithm\n",
      "#Note: Does not give a good result!\n",
      "\n",
      "nodes = [\"t. m. krishna\", \"abhogi\", \"vijayanagara musicological nonet\", \"chennai\", \n",
      "         \"azam jah of the carnatic\", \"mazhavaraayas\", \"national indian music competition\", \n",
      "         \"ceylon defence force\", \"madanakamarajan\", \"gaadha\"]\n",
      "\n",
      "target_node = \"carnatic music\"\n",
      "thresh = 0.3\n",
      "\n",
      "for src_node in nodes:\n",
      "    print src_node\n",
      "    paths = nx.shortest_paths.all_shortest_paths(g, src_node, target_node)\n",
      "    #paths = nx.simple_paths.all_simple_paths(g, src_node, target_node, cutoff=2)\n",
      "    \n",
      "    relevance = 0\n",
      "    count = 0\n",
      "    for path in paths:\n",
      "        #print path, \n",
      "        l = len(path)\n",
      "        count += 1\n",
      "        weights = []\n",
      "        for i in xrange(l-1):\n",
      "            edge_data = g.get_edge_data(path[i], path[i+1])\n",
      "            if type(edge_data) is dict:\n",
      "                weights.append(edge_data[\"weight\"])\n",
      "        \n",
      "        w = reduce(lambda x, y: x*y, weights)\n",
      "        relevance += w/(l*l)\n",
      "        #print weights, w/len(path)\n",
      "    relevance /= count\n",
      "    print \"Total relevance\", relevance, \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}