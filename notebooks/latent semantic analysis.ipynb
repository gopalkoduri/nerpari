{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from os import chdir\n",
      "chdir(\"/homedtic/gkoduri/workspace/relation-extraction/src/\")\n",
      "\n",
      "from gensim import corpora, models, similarities\n",
      "import nltk\n",
      "import networkx as nx\n",
      "import wiki_indexer as wi\n",
      "import wiki_grapher as wg\n",
      "import pickle\n",
      "reload(wi)\n",
      "reload(wg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<module 'wiki_grapher' from 'wiki_grapher.pyc'>"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki_index = pickle.load(file('/homedtic/gkoduri/workspace/relation-extraction/data/wiki_index.pickle'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "link_index = pickle.load(file('/homedtic/gkoduri/workspace/relation-extraction/data/jazz_music_hyperlinks.pickle'))\n",
      "page_titles = link_index.keys()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class WikiData():\n",
      "    def __init__(self, pages, wiki_index):\n",
      "        self.pages = pages\n",
      "        self.wiki_index = wiki_index\n",
      "        self.stemming = True\n",
      "        self.stopword_removal = True\n",
      "\n",
      "        self.alphabetic_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
      "        self.stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
      "        \n",
      "        self.content = \"\"\n",
      "        self.tokens = []\n",
      "        \n",
      "    def __iter__(self):\n",
      "        for page in self.pages:\n",
      "            self.content = wi.get_page_content(page, self.wiki_index)\n",
      "            self.tokenize()\n",
      "            yield self.tokens\n",
      "            \n",
      "    def tokenize(self):\n",
      "        #Tokenize the text to words\n",
      "        tokenized_text = [self.alphabetic_tokenizer.tokenize(s) for s in nltk.sent_tokenize(self.content)]\n",
      "        tokenized_text = np.concatenate(tokenized_text)\n",
      "\n",
      "        #Do stemming and remove stopwords\n",
      "        if self.stemming:\n",
      "            tokenized_text = [self.stemmer.stem(w) for w in tokenized_text if\n",
      "                              not w in nltk.corpus.stopwords.words('english')]\n",
      "        elif self.stopword_removal:\n",
      "            tokenized_text = [w for w in tokenized_text if not w in nltk.corpus.stopwords.words('english')]\n",
      "            \n",
      "        rare_tokens = set(w for w in set(tokenized_text) if tokenized_text.count(w) == 1)\n",
      "        tokenized_text = [w for w in tokenized_text if w not in rare_tokens]\n",
      "        \n",
      "        self.tokens = tokenized_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = WikiData(page_titles, wiki_index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary = corpora.Dictionary(tokens for tokens in data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dictionary.save('/homedtic/gkoduri/workspace/relation-extraction/data/content-analysis/jazz_music/lsa/jazz_music.dict')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dictionary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class WikiCorpus():\n",
      "    def __init__(self, data, dictionary):\n",
      "        self.dictionary = dictionary\n",
      "        self.data = data\n",
      "    def __iter__(self):\n",
      "        for tokens in self.data:\n",
      "            yield self.dictionary.doc2bow(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_gen = WikiCorpus(data, dictionary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus = [i for i in corpus_gen]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpora.MmCorpus.serialize('/homedtic/gkoduri/workspace/relation-extraction/data/content-analysis/jazz_music/lsa/jazz_music.dict.mm', corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tfidf = models.TfidfModel(corpus)\n",
      "corpus_tfidf = tfidf[corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi.print_topics(20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = similarities.MatrixSimilarity(lsi[corpus_tfidf])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange(0, 40):\n",
      "    sims = index[lsi[corpus[i]]]\n",
      "    sims[i] = 0\n",
      "    sim_index = list(enumerate(sims))\n",
      "    sim_index = sorted(sim_index, key=lambda x: x[1], reverse=True)\n",
      "    print page_titles[i], \":\"\n",
      "    for j, sim_value in sim_index[:10]:\n",
      "        print page_titles[j], sim_value\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsa_g = wg.graph_lsa(page_titles, dictionary, corpus, num_topics=300, num_neighbors=100, sim_thresh=0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(page_titles)\n",
      "print lsa_g.number_of_nodes(), lsa_g.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "annotations = pickle.load(file('/homedtic/gkoduri/workspace/relation-extraction/data/annotations.pickle'))\n",
      "for i in set(page_titles)-set(lsa_g.nodes()):\n",
      "    if i in annotations.keys():\n",
      "        print i, annotations[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_graphml(lsa_g, '/homedtic/gkoduri/workspace/relation-extraction/data/india_politic_lsa.graphml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graph_utils as gu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filt_g = gu.filter_edgeweight(lsa_g, 0.6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print filt_g.number_of_nodes(), filt_g.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filt_g_8 = gu.filter_edgeweight(filt_g, 0.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print filt_g_8.number_of_nodes(), filt_g_8.number_of_edges()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_graphml(filt_g_8, '/homedtic/gkoduri/workspace/relation-extraction/data/india_politic_lsa-0.8.graphml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"hello\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    }
   ],
   "metadata": {}
  }
 ]
}